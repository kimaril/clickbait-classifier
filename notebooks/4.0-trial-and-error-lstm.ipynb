{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from razdel import sentenize\n",
    "import zipfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные: решила оставить то, что насобирала и наразмечала. Три класса: unrelated, journalism и clickbait. Попробовала ROUGE, остальные метрики неоч для первой итерации, они вообще из других тематик (CIDEr - компютервижн, расстояние Левенштейна - ЗАЧЕМ это же редакторское расстояние, оно не отражает семантику текста, имхо, BLEU сравнивает человеческий перевод с машинным, тоже мимо). <br/>\n",
    "Модель: будем фигачить лстм по образу и подобию того, что накатали португальцы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(0)\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import itertools\n",
    "import collections\n",
    "import unicodedata\n",
    "import keras\n",
    "from sklearn import preprocessing\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Masking , Lambda , Dense, Dropout, Embedding, LSTM, GRU, Recurrent, Bidirectional, Layer, GlobalMaxPooling1D, Input, Permute, Highway, TimeDistributed\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_seq_len = 50\n",
    "hidden_units = 300\n",
    "\n",
    "class GlobalMaxPooling1DMasked(GlobalMaxPooling1D):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        super(GlobalMaxPooling1DMasked, self).__init__(**kwargs)\n",
    "    def build(self, input_shape): super(GlobalMaxPooling1DMasked, self).build(input_shape)\n",
    "    def call(self, x, mask=None): return super(GlobalMaxPooling1DMasked, self).call(x)\n",
    "\n",
    "class SelfAttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.attention = None\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        super(SelfAttLayer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='kernel', shape=(input_shape[-1],), initializer='normal', trainable=True)\n",
    "        super(SelfAttLayer, self).build(input_shape)\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        #eij = K.tanh(K.squeeze(K.dot(x, K.expand_dims(self.W)), axis=-1))\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        #weights = ai/K.expand_dims(K.sum(ai, axis=1), 1)\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        #weighted_input = x*K.expand_dims(weights,2)\n",
    "        self.attention = weights\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "        #return weights\n",
    "    def get_output_shape_for(self, input_shape): return (input_shape[0], input_shape[-1])\n",
    "    #def get_output_shape_for(self, input_shape): return (input_shape[0], input_shape[-2], 1)\n",
    "def compute_output_shape(self, input_shape): return self.get_output_shape_for(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_preprocessing import Preprocessor\n",
    "from plot_tools import plot_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/external/stopwords.txt\", mode=\"r\") as f:\n",
    "    stopwords = [s.replace(\"\\n\", \"\") for s in f.readlines()]\n",
    "    \n",
    "stopwords.remove(\"не\") # несильно влияет на качество, но для отдельных примеров мб решающим\n",
    "stopwords.remove(\"нет\")\n",
    "\n",
    "p = Preprocessor(stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"../data/interim/training-gbc-normalized-02-04.json\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e8d9bd0aed4145a90fe2d504b683e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='source', options=('interfax.ru', 'dni.ru', 'utro.ru', 'artificial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def show_by_source(source=data.source.unique()):\n",
    "    df=data\n",
    "    print(\"Total number of posts: {}\".format(df.shape[0]))\n",
    "    print(\"Number of posts from source {0}: {1}\".format(source, df[df[\"source\"]==source].shape[0]))\n",
    "    return df[df[\"source\"]==source].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_texts = data.text.tolist()[:20]\n",
    "twenty_titles = data.title.tolist()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lead = lambda body : \" \".join([substring.text for substring in list(sentenize(body))[:2]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ae2de44cf04050ae505813d2f65913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Extracting lead', max=12575, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tqdm_notebook.pandas(desc=\"Extracting lead\")\n",
    "\n",
    "data[\"lead_sentence\"] = data[\"text\"].progress_apply(get_lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8fd6ce64834d6c8aca9ed062844ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Normalizing lead', max=12575, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tqdm_notebook.pandas(desc=\"Normalizing lead\")\n",
    "\n",
    "data[\"lead_norm\"] = data[\"lead_sentence\"].progress_apply(lambda x : p.beautify(x, normalize_text=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.31249999736328127, 'p': 1.0, 'r': 0.18518518518518517},\n",
       "  'rouge-2': {'f': 0.2580645138813736, 'p': 1.0, 'r': 0.14814814814814814},\n",
       "  'rouge-l': {'f': 0.1903271405493217, 'p': 1.0, 'r': 0.18518518518518517}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(data.title_norm.values[0], data.lead_norm.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       "  'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       "  'rouge-l': {'f': 0.0, 'p': 0.0, 'r': 0.0}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(p.beautify(\"В Сочи пройдет зимняя Олимпиада\", normalize_text=True),\n",
    "                 p.beautify(\"Спортивные соревнования предложили вновь провести на российском курорте \\\n",
    "                 В 2026 г. на Красной Поляне могут пройти олимпийские соревнования по бобслею,\\\n",
    "                 скелетону и санному спорту.\", normalize_text=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'спортивный соревнование предлагать вновь проводить российский курорт красный поляна мочь проходить олимпийский соревнование бобслей скелетон санный спорт'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.beautify(\"Спортивные соревнования предложили вновь провести на российском курорте \\\n",
    "                 В 2026 г. на Красной Поляне могут пройти олимпийские соревнования по бобслею,\\\n",
    "                 скелетону и санному спорту.\", normalize_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.22222221944444445,\n",
       "   'p': 0.13333333333333333,\n",
       "   'r': 0.6666666666666666},\n",
       "  'rouge-2': {'f': 0.05263157666204997, 'p': 0.030303030303030304, 'r': 0.2},\n",
       "  'rouge-l': {'f': 0.13756613756592023,\n",
       "   'p': 0.13333333333333333,\n",
       "   'r': 0.6666666666666666}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(p.beautify(\"Суд в США оставил обвиняемую в шпионаже россиянку под стражей из-за «риска побега» \\\n",
    "                 Россиянку Марию Бутину, задержанную в Вашингтоне по подозрению в шпионаже, оставили \\\n",
    "                 под стражей до начала судебного разбирательства по ее делу, однако адвокат Нил Дрисколл не сомневается, \\\n",
    "                 что ему удастся доказать невиновность своей подзащитной, и в итоге девушку освободят.\", normalize_text=True),\n",
    "                 p.beautify(\"Адвокат уверен, что он докажет невиновность Бутиной, и ее освободят\", normalize_text=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       "  'rouge-2': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
       "  'rouge-l': {'f': 0.0, 'p': 0.0, 'r': 0.0}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bait_head = \"Звезду первого советского фильма ужасов убило проклятье\"\n",
    "bait_body = \"В студенческие годы она напророчила Анатолию Ромашину трех жен, \\\n",
    "            столько же детей (двух дочерей и сына) и смерть в 83 года. \\\n",
    "            Сбылось все, кроме последнего. Актер погиб не старым – в 69 лет. \\\n",
    "            А виною всему, по мнению поклонников, – его роковые роли.\"\n",
    "\n",
    "rouge.get_scores(p.beautify(bait_body, normalize_text=True),\n",
    "                 p.beautify(bait_head, normalize_text=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rouge_scores(headlines, articles):\n",
    "    rouge_vect = []\n",
    "    fails = 0\n",
    "    for h, a in tqdm(zip(headlines, articles)):\n",
    "        rouge_values = []\n",
    "        try:\n",
    "            scores = rouge.get_scores(h, a)\n",
    "            rouge_values += [scores[0]['rouge-1']['f']]\n",
    "            rouge_values += [scores[0]['rouge-2']['f']]\n",
    "            rouge_values += [scores[0]['rouge-l']['f']]\n",
    "        except:\n",
    "            rouge_values = [0,0,0]\n",
    "            fails += 1\n",
    "        rouge_vect += [rouge_values]\n",
    "    print(\"Number of fails: {}\".format(fails))\n",
    "    return rouge_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12575it [00:02, 4363.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fails: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data[\"head_lead_rouge\"]= get_rouge_scores(data.title_norm.tolist(), data.lead_norm.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12575it [00:11, 1066.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fails: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data[\"head_body_rouge\"]= get_rouge_scores(data.title_norm.tolist(), data.text_norm.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>text_norm</th>\n",
       "      <th>title</th>\n",
       "      <th>title_norm</th>\n",
       "      <th>url</th>\n",
       "      <th>lead_sentence</th>\n",
       "      <th>lead_norm</th>\n",
       "      <th>head_lead_rouge</th>\n",
       "      <th>head_body_rouge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>124272138</td>\n",
       "      <td>Journalism</td>\n",
       "      <td>interfax.ru</td>\n",
       "      <td>Россия поставила Анголе четыре истреьителя Су-...</td>\n",
       "      <td>россия поставлять ангола четыре истреьитель ко...</td>\n",
       "      <td>Россия поставила Анголе четыре истребителя Су-...</td>\n",
       "      <td>россия поставлять ангола четыре истребитель</td>\n",
       "      <td>http://www.interfax.ru/world/638657</td>\n",
       "      <td>Россия поставила Анголе четыре истреьителя Су-...</td>\n",
       "      <td>россия поставлять ангола четыре истреьитель ко...</td>\n",
       "      <td>[0.31249999736328127, 0.2580645138813736, 0.19...</td>\n",
       "      <td>[0.09708737771703273, 0.05925925868422497, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124272120</td>\n",
       "      <td>Journalism</td>\n",
       "      <td>interfax.ru</td>\n",
       "      <td>Нерезиденты и дочерние иностранные банки в окт...</td>\n",
       "      <td>нерезидент дочерний иностранный банк октябрь с...</td>\n",
       "      <td>Нерезиденты в октябре купили ОФЗ на аукционах ...</td>\n",
       "      <td>нерезидент октябрь купить офз аукцион минфин м...</td>\n",
       "      <td>http://www.interfax.ru/business/638658</td>\n",
       "      <td>Нерезиденты и дочерние иностранные банки в окт...</td>\n",
       "      <td>нерезидент дочерний иностранный банк октябрь с...</td>\n",
       "      <td>[0.3030302993572085, 0.06249999658203144, 0.21...</td>\n",
       "      <td>[0.12727272592396696, 0.036809815128909656, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111233217</td>\n",
       "      <td>Journalism</td>\n",
       "      <td>interfax.ru</td>\n",
       "      <td>Северо-Кавказский окружной военный суд вынес о...</td>\n",
       "      <td>окружной военный суд вынести обвинительный при...</td>\n",
       "      <td>Житель Ростовской области осужден на 11 лет за...</td>\n",
       "      <td>житель ростовский область осуждать год подгото...</td>\n",
       "      <td>http://www.interfax.ru/russia/627809</td>\n",
       "      <td>Северо-Кавказский окружной военный суд вынес о...</td>\n",
       "      <td>окружной военный суд вынести обвинительный при...</td>\n",
       "      <td>[0.19999999711250002, 0.05263157628808878, 0.0...</td>\n",
       "      <td>[0.102040815, 0.01904761797006809, 0.055245027...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110816371</td>\n",
       "      <td>Journalism</td>\n",
       "      <td>interfax.ru</td>\n",
       "      <td>Футболисты московского \"Динамо\" нанесли пораже...</td>\n",
       "      <td>футболист московский динамо наносить поражение...</td>\n",
       "      <td>Динамо\" обыграло \"Оренбург\" в матче чемпионата...</td>\n",
       "      <td>динамо обыгрывать оренбург матч чемпионат росс...</td>\n",
       "      <td>http://www.interfax.ru/sport/627601</td>\n",
       "      <td>Футболисты московского \"Динамо\" нанесли пораже...</td>\n",
       "      <td>футболист московский динамо наносить поражение...</td>\n",
       "      <td>[0.2499999958680556, 0.0, 0.1929223744293267]</td>\n",
       "      <td>[0.13953488099513256, 0.0, 0.08585289048708412]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121896641</td>\n",
       "      <td>Journalism</td>\n",
       "      <td>interfax.ru</td>\n",
       "      <td>Движение FEMEN взяло на себя ответственность з...</td>\n",
       "      <td>движение femen взять ответственность акция про...</td>\n",
       "      <td>Активистки FEMEN провели в Париже акцию против...</td>\n",
       "      <td>активистка femen проводить париж акция против ...</td>\n",
       "      <td>http://www.interfax.ru/world/637359</td>\n",
       "      <td>Движение FEMEN взяло на себя ответственность з...</td>\n",
       "      <td>движение femen взять ответственность акция про...</td>\n",
       "      <td>[0.3749999965820313, 0.05882352650519045, 0.16...</td>\n",
       "      <td>[0.2105263136349646, 0.029850744638004103, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id       label       source  \\\n",
       "0  124272138  Journalism  interfax.ru   \n",
       "1  124272120  Journalism  interfax.ru   \n",
       "2  111233217  Journalism  interfax.ru   \n",
       "3  110816371  Journalism  interfax.ru   \n",
       "4  121896641  Journalism  interfax.ru   \n",
       "\n",
       "                                                text  \\\n",
       "0  Россия поставила Анголе четыре истреьителя Су-...   \n",
       "1  Нерезиденты и дочерние иностранные банки в окт...   \n",
       "2  Северо-Кавказский окружной военный суд вынес о...   \n",
       "3  Футболисты московского \"Динамо\" нанесли пораже...   \n",
       "4  Движение FEMEN взяло на себя ответственность з...   \n",
       "\n",
       "                                           text_norm  \\\n",
       "0  россия поставлять ангола четыре истреьитель ко...   \n",
       "1  нерезидент дочерний иностранный банк октябрь с...   \n",
       "2  окружной военный суд вынести обвинительный при...   \n",
       "3  футболист московский динамо наносить поражение...   \n",
       "4  движение femen взять ответственность акция про...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Россия поставила Анголе четыре истребителя Су-...   \n",
       "1  Нерезиденты в октябре купили ОФЗ на аукционах ...   \n",
       "2  Житель Ростовской области осужден на 11 лет за...   \n",
       "3  Динамо\" обыграло \"Оренбург\" в матче чемпионата...   \n",
       "4  Активистки FEMEN провели в Париже акцию против...   \n",
       "\n",
       "                                          title_norm  \\\n",
       "0        россия поставлять ангола четыре истребитель   \n",
       "1  нерезидент октябрь купить офз аукцион минфин м...   \n",
       "2  житель ростовский область осуждать год подгото...   \n",
       "3  динамо обыгрывать оренбург матч чемпионат росс...   \n",
       "4  активистка femen проводить париж акция против ...   \n",
       "\n",
       "                                      url  \\\n",
       "0     http://www.interfax.ru/world/638657   \n",
       "1  http://www.interfax.ru/business/638658   \n",
       "2    http://www.interfax.ru/russia/627809   \n",
       "3     http://www.interfax.ru/sport/627601   \n",
       "4     http://www.interfax.ru/world/637359   \n",
       "\n",
       "                                       lead_sentence  \\\n",
       "0  Россия поставила Анголе четыре истреьителя Су-...   \n",
       "1  Нерезиденты и дочерние иностранные банки в окт...   \n",
       "2  Северо-Кавказский окружной военный суд вынес о...   \n",
       "3  Футболисты московского \"Динамо\" нанесли пораже...   \n",
       "4  Движение FEMEN взяло на себя ответственность з...   \n",
       "\n",
       "                                           lead_norm  \\\n",
       "0  россия поставлять ангола четыре истреьитель ко...   \n",
       "1  нерезидент дочерний иностранный банк октябрь с...   \n",
       "2  окружной военный суд вынести обвинительный при...   \n",
       "3  футболист московский динамо наносить поражение...   \n",
       "4  движение femen взять ответственность акция про...   \n",
       "\n",
       "                                     head_lead_rouge  \\\n",
       "0  [0.31249999736328127, 0.2580645138813736, 0.19...   \n",
       "1  [0.3030302993572085, 0.06249999658203144, 0.21...   \n",
       "2  [0.19999999711250002, 0.05263157628808878, 0.0...   \n",
       "3      [0.2499999958680556, 0.0, 0.1929223744293267]   \n",
       "4  [0.3749999965820313, 0.05882352650519045, 0.16...   \n",
       "\n",
       "                                     head_body_rouge  \n",
       "0  [0.09708737771703273, 0.05925925868422497, 0.0...  \n",
       "1  [0.12727272592396696, 0.036809815128909656, 0....  \n",
       "2  [0.102040815, 0.01904761797006809, 0.055245027...  \n",
       "3    [0.13953488099513256, 0.0, 0.08585289048708412]  \n",
       "4  [0.2105263136349646, 0.029850744638004103, 0.0...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"id\", \"label\", \"source\", \"text\", \"title\", \"url\"]\n",
    "data[columns].to_json(\"../data/raw/final-raw-for-senior-thesis.json\", force_ascii=False, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, Embedding, LSTM, Masking, Dense, Dropout\n",
    "from keras.layers import GlobalMaxPooling1D, Input, TimeDistributed\n",
    "from keras.layers import concatenate, multiply, subtract\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_sents = average number of sentences in article + 2\\*sigma <br/>\n",
    "max_seq_len = average length of sample concat(headlines, leads) + 2\\*sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lead_len\"] = data.lead_norm.apply(lambda x : len(x.split()))\n",
    "data[\"text_sent_number\"] = data.text.apply(lambda body : len([substring.text for substring in list(sentenize(body))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get maximum sequence length\n",
    "avg_lead = data.lead_len.mean()\n",
    "std_lead = data.lead_len.std()\n",
    "max_seq_len = int(round(avg_lead + 2*std_lead))\n",
    "\n",
    "# get maximum number of sentences in text\n",
    "avg_sents = data.text_sent_number.mean()\n",
    "std_sents = data.text_sent_number.std()\n",
    "max_sents = int(round(avg_sents + 2*std_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"../data/external/184.zip\", 'r') as archive:\n",
    "#         self.meta = json.load(archive.open(\"meta.json\"))\n",
    "        stream = archive.open('model.bin')\n",
    "        word2vec = KeyedVectors.load_word2vec_format(stream, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='loss', patience=2, verbose=1, restore_best_weights=True)\n",
    "embedding_layer = word2vec.get_keras_embedding(train_embeddings=True)\n",
    "lstm1 = LSTM(hidden_units, implementation=2, return_sequences=True, name='lstm1' )\n",
    "lstm1 = Bidirectional(lstm1, name='bilstm1')\n",
    "right_branch_lstm1 = LSTM(hidden_units, implementation=2, return_sequences=True )\n",
    "right_branch_lstm1 = Bidirectional(right_branch_lstm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Masking(mask_value=0, input_shape=(max_seq_len,))(input_headline)\n",
    "embed = embedding_layer(mask)\n",
    "l1 = lstm1(embed)\n",
    "drop1 = Dropout(0.1)(l1)\n",
    "maxim = GlobalMaxPooling1DMasked()(drop1)\n",
    "# att = SelfAttLayer(name='attention')(drop1)\n",
    "# out = concatenate([maxim, att])\n",
    "HeadlineEncoder = Model(input_headline, maxim, name='HeadlineEncoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_headline = Input(shape=(max_seq_len,))\n",
    "input_body = Input(shape=(max_sents, max_seq_len))\n",
    "input_lead = Input(shape=(max_seq_len,))\n",
    "input_overlap_body = Input(shape=(1,))\n",
    "input_overlap_lead = Input(shape=(1,))\n",
    "input_polarity_body = Input(shape=(2,))\n",
    "input_polarity_lead = Input(shape=(2,))\n",
    "input_hand_body = Input(shape=(26,))\n",
    "input_hand_lead = Input(shape=(26,))\n",
    "input_cos_body = Input(shape=(1,))\n",
    "input_cos_lead = Input(shape=(1,))\n",
    "input_rouge_body = Input(shape=(3,))\n",
    "input_rouge_lead = Input(shape=(3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-c84c90c65a3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                 input_hand_lead, input_rouge_lead])\n\u001b[1;32m     24\u001b[0m \u001b[0mdrop3_lead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_merge_lead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdense1_lead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dense1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop3_lead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mdrop4_lead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense1_lead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdense2_lead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dense2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop4_lead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layer_dict' is not defined"
     ]
    }
   ],
   "source": [
    "body_sentence = TimeDistributed(HeadlineEncoder)(input_body)\n",
    "body_g1 = right_branch_lstm1(body_sentence)\n",
    "body_g1 = Dropout(0.1)(body_g1)\n",
    "body_maxim = GlobalMaxPooling1D()(body_g1)\n",
    "# body_att = SelfAttLayer()(body_g1)\n",
    "# body_out = concatenate([body_maxim, body_att])\n",
    "DocumentEncoder = Model(input_body, body_maxim, name='DocumentEncoder')\n",
    "\n",
    "##############################\n",
    "\n",
    "# Combining both representations #\n",
    "\n",
    "headline_representation = HeadlineEncoder(input_headline)\n",
    "document_representation = DocumentEncoder(input_body)\n",
    "\n",
    "# Match between headline and first two sentences from body #\n",
    "\n",
    "lead_representation = HeadlineEncoder(input_lead)\n",
    "concat_lead = concatenate([headline_representation, lead_representation])\n",
    "mul_lead = multiply([headline_representation, lead_representation])\n",
    "dif_lead = subtract([headline_representation, lead_representation])\n",
    "final_merge_lead = concatenate([concat_lead, mul_lead, dif_lead, input_overlap_lead, input_polarity_lead, \n",
    "                                input_hand_lead, input_rouge_lead])\n",
    "drop3_lead = Dropout(0.1)(final_merge_lead)\n",
    "dense1_lead = Dense(hidden_units*2, activation='relu', weights=layer_dict['dense1'].get_weights())(drop3_lead)\n",
    "drop4_lead = Dropout(0.1)(dense1_lead)\n",
    "dense2_lead = Dense(hidden_units, activation='relu',weights=layer_dict['dense2'].get_weights())(drop4_lead)\n",
    "match = Dropout(0.1)(dense2_lead)\n",
    "# layer_dict -- это, видимо, штука с весами предобученными на MultiNLI\n",
    "#####################################################\n",
    "\n",
    "concat = concatenate([headline_representation, document_representation])\n",
    "mul = multiply([headline_representation, document_representation])\n",
    "dif = subtract([headline_representation, document_representation])\n",
    "final_merge = concatenate([concat, mul, dif, input_overlap, input_refuting, input_polarity, input_hand, input_sim, input_bleu, input_rouge, input_cider])\n",
    "drop3 = Dropout(0.1)(final_merge)\n",
    "dense1 = Dense(hidden_units*2, activation='relu', name='dense1', weights=layer_dict['dense1'].get_weights())(drop3)\n",
    "drop4 = Dropout(0.1)(dense1)\n",
    "dense2 = Dense(hidden_units, activation='relu', name='dense2', weights=layer_dict['dense2'].get_weights())(drop4)\n",
    "drop5 = Dropout(0.1)(dense2)\n",
    "concat_final = concatenate([drop5,match,input_talos_count, input_talos_tfidfsim, input_talos_headline_svd, input_talos_body_svd, \\\n",
    "                     input_talos_svdsim, input_talos_headline_w2v, input_talos_body_w2v, input_talos_w2vsim, \\\n",
    "                     input_talos_headline_senti, input_talos_body_senti])\n",
    "drop6 = Dropout(0.1)(concat_final)\n",
    "dense3 = Dense(4, activation='softmax')(drop6)\n",
    "final_model = Model([input_headline, input_body,input_overlap, input_refuting, input_polarity, input_hand, \\\n",
    "                     input_sim, input_sim_two, input_bleu, input_rouge,input_cider, input_two, input_overlap_two, input_refuting_two, input_polarity_two, input_hand_two, \\\n",
    "                     input_bleu_two, input_rouge_two, input_cider_two, input_talos_count, input_talos_tfidfsim, input_talos_headline_svd, input_talos_body_svd, \\\n",
    "                     input_talos_svdsim, input_talos_headline_w2v, input_talos_body_w2v, input_talos_w2vsim, \\\n",
    "input_talos_headline_senti, input_talos_body_senti], dense3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json(\"../data/interim/training-lstm-w-lengths-11-04.json\", force_ascii=False, lines=True, orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 (tf-gpu)",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
